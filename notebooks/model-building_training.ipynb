{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "200a00e4-0992-439e-8d0a-a4e3d0610a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud Class Distribution:\n",
      " class\n",
      "0    0.906354\n",
      "1    0.093646\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\10Academy1\\fraud_detection\\week-8\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['ip_int']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "D:\\10Academy1\\fraud_detection\\week-8\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['ip_int']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "D:\\10Academy1\\fraud_detection\\week-8\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['ip_int']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Logistic Regression Results:\n",
      "[[34084   156]\n",
      " [ 2192  1346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97     34240\n",
      "           1       0.90      0.38      0.53      3538\n",
      "\n",
      "    accuracy                           0.94     37778\n",
      "   macro avg       0.92      0.69      0.75     37778\n",
      "weighted avg       0.94      0.94      0.93     37778\n",
      "\n",
      "F1 Score: 0.5341269841269841\n",
      "Average Precision (AUC-PR): 0.5812689702978775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\10Academy1\\fraud_detection\\week-8\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['ip_int']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "D:\\10Academy1\\fraud_detection\\week-8\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['ip_int']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost...\n",
      "\n",
      "ðŸ“Š XGBoost Results:\n",
      "[[30351  3889]\n",
      " [ 2166  1372]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91     34240\n",
      "           1       0.26      0.39      0.31      3538\n",
      "\n",
      "    accuracy                           0.84     37778\n",
      "   macro avg       0.60      0.64      0.61     37778\n",
      "weighted avg       0.87      0.84      0.85     37778\n",
      "\n",
      "F1 Score: 0.3118536197295147\n",
      "Average Precision (AUC-PR): 0.3210012772678157\n",
      "\n",
      "ðŸ“Œ Model Comparison:\n",
      "Logistic Regression â†’ F1: 0.5341, AUC-PR: 0.5813\n",
      "XGBoost             â†’ F1: 0.3119, AUC-PR: 0.3210\n",
      "\n",
      "ðŸ† Logistic Regression outperforms XGBoost on F1 and AUC-PR metrics.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory containing the script to sys.path\n",
    "sys.path.append(os.path.abspath('../src'))  # Adjust the path to point to the directory containing fraud_detection.py\n",
    "\n",
    "# Import the function from the script\n",
    "from fraud_detection import run_fraud_detection\n",
    "\n",
    "# Call the function with the path to your dataset\n",
    "run_fraud_detection('../data/fraud_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f417f898-f8fd-448a-a2dc-f0d781ce03b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\10Academy1\\fraud_detection\\week-8\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['ip_int']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "D:\\10Academy1\\fraud_detection\\week-8\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['ip_int']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Logistic Regression Results:\n",
      "[[26964   429]\n",
      " [ 1249  1581]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     27393\n",
      "           1       0.79      0.56      0.65      2830\n",
      "\n",
      "    accuracy                           0.94     30223\n",
      "   macro avg       0.87      0.77      0.81     30223\n",
      "weighted avg       0.94      0.94      0.94     30223\n",
      "\n",
      "F1 Score: 0.653305785123967\n",
      "Average Precision (AUC-PR): 0.5745924264311452\n",
      "Training XGBoost...\n",
      "XGBoost Results:\n",
      "[[24039  3354]\n",
      " [ 1740  1090]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90     27393\n",
      "           1       0.25      0.39      0.30      2830\n",
      "\n",
      "    accuracy                           0.83     30223\n",
      "   macro avg       0.59      0.63      0.60     30223\n",
      "weighted avg       0.87      0.83      0.85     30223\n",
      "\n",
      "F1 Score: 0.2996975529282376\n",
      "Average Precision (AUC-PR): 0.330638243855421\n",
      "\n",
      "Model Comparison:\n",
      "Logistic Regression â†’ F1: 0.6533, AUC-PR: 0.5746\n",
      "XGBoost             â†’ F1: 0.2997, AUC-PR: 0.3306\n",
      "âœ… Models and preprocessing pipeline saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory containing the script to sys.path\n",
    "sys.path.append(os.path.abspath('../src'))  # Adjust the path to your script directory\n",
    "\n",
    "# Import the function from the script\n",
    "from data_preparation import run_fraud_detection\n",
    "\n",
    "# Define paths\n",
    "data_path = '../data/fraud_cleaned.csv'  # Adjust according to your data location\n",
    "model_dir = 'D:/10Academy1/fraud_detection/models'  # Adjust as needed\n",
    "\n",
    "# Call the function\n",
    "run_fraud_detection(data_path, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f266a2be-d0d4-450a-94bd-dc8958e5c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      " Class\n",
      "0    0.998273\n",
      "1    0.001727\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "ðŸ”¹ Logistic Regression Results:\n",
      "[[69444  1635]\n",
      " [   14   109]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     71079\n",
      "           1       0.06      0.89      0.12       123\n",
      "\n",
      "    accuracy                           0.98     71202\n",
      "   macro avg       0.53      0.93      0.55     71202\n",
      "weighted avg       1.00      0.98      0.99     71202\n",
      "\n",
      "F1 Score: 0.11676486341724691\n",
      "Average Precision (AUC-PR): 0.7038566346457426\n",
      "\n",
      "ðŸ”¹ XGBoost Results:\n",
      "[[70894   185]\n",
      " [   18   105]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71079\n",
      "           1       0.36      0.85      0.51       123\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       0.68      0.93      0.75     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n",
      "F1 Score: 0.5084745762711864\n",
      "Average Precision (AUC-PR): 0.8040573114816045\n",
      "\n",
      "ðŸ“Œ Credit Card Dataset - Model Comparison:\n",
      "Logistic Regression â†’ F1: 0.1168, AUC-PR: 0.7039\n",
      "XGBoost             â†’ F1: 0.5085, AUC-PR: 0.8041\n",
      "\n",
      "ðŸ† XGBoost outperforms Logistic Regression on both F1 and AUC-PR metrics.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory containing the script to sys.path\n",
    "sys.path.append(os.path.abspath('../src'))  # Adjust the path to your script directory\n",
    "\n",
    "# Import the function from the script\n",
    "from models_taining import run_fraud_detection\n",
    "\n",
    "# Call the function with the path to your dataset\n",
    "data_path = '../data/creditcard.csv'  # Adjust according to your data location\n",
    "run_fraud_detection(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "070d4c31-8814-4653-b9ed-cd84e0d768f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "\n",
      "Logistic Regression Results:\n",
      "[[55397  1467]\n",
      " [    8    90]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     56864\n",
      "           1       0.06      0.92      0.11        98\n",
      "\n",
      "    accuracy                           0.97     56962\n",
      "   macro avg       0.53      0.95      0.55     56962\n",
      "weighted avg       1.00      0.97      0.99     56962\n",
      "\n",
      "F1 Score: 0.10876132930513595\n",
      "Average Precision (AUC-PR): 0.724469435669471\n",
      "\n",
      "Starting XGBoost hyperparameter tuning...\n",
      "\n",
      "Best XGBoost params: {'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'scale_pos_weight': np.float64(1.0), 'seed': 42, 'max_depth': 6, 'eta': 0.1, 'subsample': 0.8, 'colsample_bytree': 1, 'verbosity': 0}\n",
      "Best XGBoost F1 at threshold 0.5: 0.5260\n",
      "Optimal threshold based on max F1: 0.9809\n",
      "\n",
      "XGBoost Results with tuned threshold:\n",
      "[[56858     6]\n",
      " [   20    78]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.93      0.80      0.86        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.96      0.90      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "F1 Score: 0.8571428571428571\n",
      "Average Precision (AUC-PR): 0.8465667518482527\n",
      "\n",
      "Model Comparison:\n",
      "Logistic Regression â†’ F1: 0.1088, AUC-PR: 0.7245\n",
      "XGBoost (tuned)    â†’ F1: 0.8571, AUC-PR: 0.8466\n",
      "\n",
      "ðŸ† Best Model Selected: XGBoost (tuned)\n",
      "âœ… Saved: XGBoost (tuned) to ../models/credit_model_xgb.json\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory containing the script to sys.path\n",
    "sys.path.append(os.path.abspath('../src'))  # Adjust the path to your script directory\n",
    "\n",
    "# Import the function from the script\n",
    "from model_evaluation import run_fraud_detection\n",
    "\n",
    "# Define paths\n",
    "data_path = '../data/creditcard.csv'  # Adjust according to your data location\n",
    "model_save_path = '../models'  # Adjust as needed\n",
    "\n",
    "# Call the function\n",
    "run_fraud_detection(data_path, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32ddd3dc-a367-4cb5-a1c0-8ab0ac080581",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_importance\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m plot_importance(\u001b[43mxgb_model\u001b[49m, max_num_features=\u001b[32m10\u001b[39m)\n\u001b[32m      6\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mXGBoost Feature Importance\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m plt.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'xgb_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Example for XGBoost\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "\n",
    "plot_importance(xgb_model, max_num_features=10)\n",
    "plt.title(\"XGBoost Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e20e0-c653-4c90-8eb1-c75475923a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (week-8)",
   "language": "python",
   "name": "week-8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
